{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVMeefqY7bU0oE8Nkao8sx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shridharpawar77/Natural-Language-Processing/blob/master/Finetune_GPT_2_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the Pre-trained Language Model and Tokenizer"
      ],
      "metadata": {
        "id": "0HpQIn1CYzZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "\n",
        "# Load the pre-trained tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the pre-trained model for sequence classification\n",
        "model = GPT2ForSequenceClassification.from_pretrained('openai-community/gpt2')\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "Q8wwglvnSLx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbce142-2e05-48b2-9267-9816056f4101"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare the Instruction Data and Sentiment Analysis Dataset"
      ],
      "metadata": {
        "id": "Wv_2hReeY5vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"I loved the resort. The pool actiities and the Barbique event was managed well!\",\n",
        "         \"The dining service was terrible, one has to wait for too long\",\n",
        "         \"The rooms are also good.\"]\n",
        "sentiments = [\"positive\", \"negative\", \"positive\"]\n",
        "instructions = [\"Analyze the sentiment of the text and identify if it is positive.\",\n",
        "                \"Analyze the sentiment of the text and identify if it is negative.\",\n",
        "                \"Analyze the sentiment of the text and identify if it is positive.\"]"
      ],
      "metadata": {
        "id": "yOD2ZuHWTxEy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: tokenize the texts, sentiments, and instructions using the tokenizer:"
      ],
      "metadata": {
        "id": "9BuupeCHY-n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the texts, sentiments, and instructions\n",
        "encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_instructions = tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Extract input IDs, attention masks, and instruction IDs\n",
        "input_ids = encoded_texts['input_ids']\n",
        "attention_mask = encoded_texts['attention_mask']\n",
        "instruction_ids = encoded_instructions['input_ids']\n",
        "\n",
        "# Convert the sentiment labels to numerical form\n",
        "sentiment_labels = [sentiments.index(sentiment) for sentiment in sentiments]"
      ],
      "metadata": {
        "id": "am0irfG3T3IS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Customize the Model Architecture with Instructions"
      ],
      "metadata": {
        "id": "N75YCLUBZE1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Concatenate instruction IDs with input IDs and adjust attention mask\n",
        "input_ids = torch.cat([instruction_ids, input_ids], dim=1)\n",
        "attention_mask = torch.cat([torch.ones_like(instruction_ids), attention_mask], dim=1)"
      ],
      "metadata": {
        "id": "Cze3FmSRT8oP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Fine-Tune the Model with Instructions"
      ],
      "metadata": {
        "id": "BTE1KFSjZIl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Fine-tune the model\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=torch.tensor(sentiment_labels))\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "N3dftzFuUKHM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7lWLPocb0UU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}